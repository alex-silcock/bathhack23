{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\natha\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "pygame 2.3.0 (SDL 2.24.2, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Requirement already satisfied: opencv-python in c:\\users\\natha\\anaconda3\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\natha\\anaconda3\\lib\\site-packages (0.9.0.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\natha\\anaconda3\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\natha\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\natha\\anaconda3\\lib\\site-packages (from mediapipe) (4.7.0.68)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from mediapipe) (22.12.6)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from mediapipe) (3.19.6)\n",
      "Requirement already satisfied: absl-py in c:\\users\\natha\\anaconda3\\lib\\site-packages (from mediapipe) (1.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\natha\\anaconda3\\lib\\site-packages (0.2.13)\n",
      "Requirement already satisfied: pysinewave in c:\\users\\natha\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: sounddevice in c:\\users\\natha\\anaconda3\\lib\\site-packages (from pysinewave) (0.4.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\natha\\anaconda3\\lib\\site-packages (from pysinewave) (1.21.5)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\natha\\anaconda3\\lib\\site-packages (from sounddevice->pysinewave) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\natha\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice->pysinewave) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "# tracking stuff\n",
    "!pip install  opencv-python mediapipe sklearn matplotlib \n",
    "#tensorflow-gpu==2.4.1\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "\n",
    "# sound stuff\n",
    "!pip install pyaudio\n",
    "!pip install pysinewave\n",
    "import time\n",
    "import pyaudio\n",
    "from pysinewave import SineWave\n",
    "import pyaudio as p\n",
    "import math\n",
    "import threading\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # # Draw face connections\n",
    "    # mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "    #                          ) \n",
    "    # # Draw pose connections\n",
    "    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "    #                          mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "    #                          mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "    #                          ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    # pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return lh, rh\n",
    "\n",
    "\n",
    "actions = np.array(['hand', 'fist', 'thumbs_up'])\n",
    "no_sequences = 200\n",
    "sequence_length = 5\n",
    "\n",
    "# label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "# sequences, labels = [], []\n",
    "# for action in actions:\n",
    "#     for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "#         window = []\n",
    "#         for frame_num in range(sequence_length):\n",
    "#             res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "#             window.append(res)\n",
    "#         sequences.append(window)\n",
    "#         labels.append(label_map[action])\n",
    "\n",
    "\n",
    "left_model = Sequential()\n",
    "left_model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(5,63)))\n",
    "left_model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "left_model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "left_model.add(Dense(64, activation='relu'))\n",
    "left_model.add(Dense(32, activation='relu'))\n",
    "left_model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "left_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "right_model = Sequential()\n",
    "right_model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(5,63)))\n",
    "right_model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "right_model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "right_model.add(Dense(64, activation='relu'))\n",
    "right_model.add(Dense(32, activation='relu'))\n",
    "right_model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "right_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "left_model.load_weights('left_hand_data/test.h5')\n",
    "right_model.load_weights('right_hand_data/test.h5')\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sine wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_to_freq(pitch):\n",
    "    return 440 * 2 ^ ((pitch-9)/12)\n",
    "\n",
    "def freq_to_pitch(freq):\n",
    "    return 12 * math.log2(freq/440) + 9\n",
    "\n",
    "# CHORDS = {\n",
    "#     0   : [-10, -6, -4],\n",
    "#     0.1 : [-7, -4, -2],\n",
    "#     0.2 : [-5, -2, 1],\n",
    "#     0.3 : [-3, 1, 3],\n",
    "#     0.4 : [-1, 3, 5],\n",
    "#     0.5 : [0, 4, 7],\n",
    "#     0.6 : [2, 5, 9],\n",
    "#     0.7 : [3, 7, 10],\n",
    "#     0.8 : [5, 9, 12],\n",
    "#     0.9 : [7, 11, 14],\n",
    "#     1   : [9, 13, 16]\n",
    "# }\n",
    "\n",
    "CHORDS = {\n",
    "    0   : [-6, -2, 1],\n",
    "    0.1 : [-5, -1, 2],\n",
    "    0.2 : [-4, 0, 3],\n",
    "    0.3 : [-2, 2, 5],\n",
    "    0.4 : [-1, 3, 6],\n",
    "    0.5 : [0, 4, 7],\n",
    "    0.6 : [0.5, 5, 8],\n",
    "    0.7 : [2, 6, 9],\n",
    "    0.8 : [4, 8, 11],\n",
    "    0.9 : [5, 9, 12],\n",
    "    1   : [6, 10, 13]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class Waves:\n",
    "    def __init__(self):\n",
    "        self.wave1 = SineWave(pitch=0, pitch_per_second=100, decibels_per_second=100)\n",
    "        self.wave2 = SineWave(pitch=4, pitch_per_second=100, decibels_per_second=100)\n",
    "        self.wave3 = SineWave(pitch=7, pitch_per_second=100, decibels_per_second=100)\n",
    "        \n",
    "        self.waves = [self.wave1, self.wave2, self.wave3]\n",
    "\n",
    "    def get_chords(self, x_pos):\n",
    "        chord = [0, 4, 7]\n",
    "        offset = 1\n",
    "        difference = x_pos - 0.5\n",
    "\n",
    "        new_chord = []\n",
    "        for c in chord:\n",
    "            new_chord.append(c + (difference*10) * offset)\n",
    "\n",
    "        return new_chord\n",
    "\n",
    "    def update_chord(self, x_pos):\n",
    "        pitches = self.get_chords(x_pos)\n",
    "        for i in range(3):\n",
    "            self.waves[i].set_pitch(pitches[i])\n",
    "\n",
    "        self.run()\n",
    "\n",
    "    def set_volume(self, volume):\n",
    "        for wave in self.waves:\n",
    "            wave.set_volume(volume)\n",
    "\n",
    "    def run(self):\n",
    "        for wave in self.waves:\n",
    "            wave.play()\n",
    "        # time.sleep(1.5) # Comment out when done\n",
    "\n",
    "    def stop(self):\n",
    "        for wave in self.waves:\n",
    "            wave.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3179792165756226\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pygame\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Set the number of mixer channels\n",
    "pygame.mixer.set_num_channels(4)\n",
    "\n",
    "# Load the sound files\n",
    "sound1 = pygame.mixer.Sound('deez-nuts.wav')\n",
    "sound2 = pygame.mixer.Sound('choir.wav')\n",
    "sound3 = pygame.mixer.Sound('back-beat.wav')\n",
    "sound4 = pygame.mixer.Sound('augh.wav')\n",
    "\n",
    "print(sound1.get_length())\n",
    "\n",
    "channel1 = pygame.mixer.Channel(0)\n",
    "channel2 = pygame.mixer.Channel(1)\n",
    "channel3 = pygame.mixer.Channel(2)\n",
    "channel4 = pygame.mixer.Channel(3)\n",
    "\n",
    "# Define a function to play sounds based on input\n",
    "def play_sound(input):\n",
    "    if input == 1 and not channel1.get_busy():\n",
    "        # channel = pygame.mixer.Channel(0)\n",
    "        channel1.play(sound1)\n",
    "        # channel1.fadeout(int(sound1.get_length()*1000 - 2500))\n",
    "    elif input == 2 and not channel2.get_busy():\n",
    "        # channel = pygame.mixer.Channel(1)\n",
    "        channel2.play(sound2)\n",
    "        # channel2.fadeout(int(sound2.get_length()*1000 - 30))\n",
    "    elif input == 3 and not channel3.get_busy():\n",
    "        # channel = pygame.mixer.Channel(2)\n",
    "        channel3.play(sound3)\n",
    "        # channel3.fadeout(int(sound3.get_length()*1000 - 30))\n",
    "    elif input == 4 and not channel4.get_busy():\n",
    "        # channel = pygame.mixer.Channel(2)\n",
    "        channel4.play(sound4)\n",
    "        # channel3.fadeout(int(sound3.get_length()*1000 - 30))\n",
    "\n",
    "def update_sound(volume):\n",
    "    sound1.set_volume(volume)\n",
    "    sound2.set_volume(volume)\n",
    "    sound3.set_volume(volume)\n",
    "    sound4.set_volume(volume)\n",
    "update_sound(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_sound(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "THEREMIN_MODE = True\n",
    "RANDOM_NOISE_MODE = False\n",
    "\n",
    "# 1. New detection variables\n",
    "left_sequence = []\n",
    "right_sequence = []\n",
    "\n",
    "threshold = 0.5\n",
    "left_sentence = 'nothing'\n",
    "right_sentence = 'nothing'\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if THEREMIN_MODE:\n",
    "    sine_wave = Waves()\n",
    "    sine_wave.run()\n",
    "\n",
    "y_pos = 0\n",
    "x_pos = 0\n",
    "sequence_length = 5\n",
    "\n",
    "count = 0\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        if RANDOM_NOISE_MODE:\n",
    "            #Prediction logic\n",
    "            left_keypoints, right_keypoints = extract_keypoints(results)\n",
    "            left_sequence.append(left_keypoints)\n",
    "            right_sequence.append(right_keypoints)\n",
    "\n",
    "            left_sequence = left_sequence[-sequence_length:]\n",
    "            right_sequence = right_sequence[-sequence_length:]\n",
    "            \n",
    "            # make left predictions\n",
    "            if len(left_sequence) == sequence_length:\n",
    "                left_res = left_model.predict(np.expand_dims(left_sequence, axis=0))[0]\n",
    "                # predictions.append(np.argmax(res))\n",
    "            \n",
    "                # check for play sound\n",
    "                left_guess = np.argmax(left_res)\n",
    "                left_sentence = 'nothing'\n",
    "                if left_res[left_guess] > 0.95:\n",
    "                    left_sentence = 'left hand'\n",
    "                    if left_guess == 1:\n",
    "                        left_sentence = 'left fist'\n",
    "                        play_sound(1)\n",
    "                    if left_guess == 2:\n",
    "                        left_sentence = 'left thumbs up'\n",
    "                        play_sound(2)\n",
    "\n",
    "            # make right predictions\n",
    "            if len(right_sequence) == sequence_length:\n",
    "                right_res = right_model.predict(np.expand_dims(right_sequence, axis=0))[0]\n",
    "                right_sentence = 'nothing'\n",
    "                # check forplay sound\n",
    "                right_guess = np.argmax(right_res)\n",
    "                if right_res[right_guess] > 0.95:\n",
    "                    right_sentence = 'right hand'\n",
    "                    if right_guess == 1:\n",
    "                        right_sentence = 'right fist'\n",
    "                        play_sound(4)\n",
    "                    if right_guess == 2:\n",
    "                        right_sentence = 'right thumbs up'\n",
    "                        play_sound(3)\n",
    "        \n",
    "        # # sinth sounds\n",
    "        if THEREMIN_MODE:\n",
    "            if results.right_hand_landmarks != None:\n",
    "                right_pos = results.right_hand_landmarks.landmark[0]\n",
    "                y_pos = right_pos.y\n",
    "            if results.right_hand_landmarks != None:\n",
    "                right_pos = results.right_hand_landmarks.landmark[0]\n",
    "                x_pos = right_pos.x\n",
    "\n",
    "            # range 0 to 50\n",
    "            y_pos = 1 - y_pos\n",
    "            volume = y_pos\n",
    "            sine_wave.set_volume(volume*0.1)\n",
    "\n",
    "            # range -10 to 10\n",
    "            x_pos = 1 - x_pos\n",
    "            x_pos = round(x_pos, 1)\n",
    "            sine_wave.update_chord(x_pos)\n",
    "        \n",
    "        # # Show to screen\n",
    "        sentence = left_sentence + ' ' + right_sentence\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "\n",
    "        count += 1\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    if THEREMIN_MODE:\n",
    "        sine_wave.stop()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
